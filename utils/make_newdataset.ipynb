{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ecbbefca-28f7-4682-83b3-730074f9abaf",
   "metadata": {},
   "source": [
    "#### 파일 실행 위치: input/code/make_newdataset.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "id": "7318ad23-7161-4e6a-b7c3-bbdfffbcc402",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import warnings\n",
    "import copy\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "id": "de42781f-5573-45af-bda5-a4f6b4cfdf96",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_new = []\n",
    "test_original = []\n",
    "for i in range(len(dataset_test_new['images'])):\n",
    "    test_new.append(dataset_test_new['images'][i]['file_name']) # 624\n",
    "for i in range(len(dataset_test_original['images'])):\n",
    "    test_original.append(dataset_test_original['images'][i]['file_name']) # 819\n",
    "    \n",
    "test_leak = list(set(test_original) - set(test_new)) # 유출된 159장 filename\n",
    "# print(len(test_new))\n",
    "# print(len(test_original))\n",
    "# print(len(test_leak))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "id": "113ce73c-7d6c-48ca-b2ba-4c4c593fc4ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path  = '../data'\n",
    "train_path = dataset_path + '/train.json'\n",
    "train_all_path = dataset_path + '/train_all.json'\n",
    "data_leak_path = dataset_path + '/batch_03/data.json'\n",
    "\n",
    "with open(train_path, 'r') as f:\n",
    "    dataset_train = json.loads(f.read())\n",
    "with open(train_all_path, 'r') as f:\n",
    "    dataset_train_all = json.loads(f.read())\n",
    "with open(data_leak_path, 'r') as f:\n",
    "    dataset_leak = json.loads(f.read())\n",
    "dataset_leak_copy = copy.deepcopy(dataset_leak)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "id": "127257f5-030e-4311-bb75-29c7a2a9393c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len(dataset_train['images']))\n",
    "# print(len(dataset_train['annotations'])) # last id: 21115\n",
    "# print(len(dataset_train_all['images']))\n",
    "# print(len(dataset_train_all['annotations'])) # last id: 26401"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "id": "58c65299-ec33-4a2a-938e-654eade01911",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge train.json\n",
    "new_images = []\n",
    "new_annotations = []\n",
    "img_id = 2617\n",
    "anno_id = 21116\n",
    "for i in range(len(dataset_leak['images'])):\n",
    "    if dataset_leak['images'][i]['file_name'] in test_leak:\n",
    "        for j in range(len(dataset_leak['annotations'])):\n",
    "            if dataset_leak['annotations'][j]['image_id'] == i and dataset_leak['annotations'][j]['category_id'] != 0:\n",
    "                dataset_leak['annotations'][j]['image_id'] = img_id\n",
    "                dataset_leak['annotations'][j]['id'] = anno_id\n",
    "                new_annotations.append(dataset_leak['annotations'][j])\n",
    "                anno_id += 1\n",
    "        dataset_leak['images'][i]['id'] = img_id\n",
    "        new_images.append(dataset_leak['images'][i])  \n",
    "        img_id += 1\n",
    "        \n",
    "del new_annotations[267] # segment labeling 안되어있는것 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "id": "130b5172-b99f-4115-b70a-a7c94a3ce1d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge train_all.json\n",
    "new_images_all = []\n",
    "new_annotations_all = []\n",
    "img_id = 3272\n",
    "anno_id = 26402\n",
    "for i in range(len(dataset_leak_copy['images'])):\n",
    "    if dataset_leak_copy['images'][i]['file_name'] in test_leak:\n",
    "        for j in range(len(dataset_leak_copy['annotations'])):\n",
    "            if dataset_leak_copy['annotations'][j]['image_id'] == i and dataset_leak_copy['annotations'][j]['category_id'] != 0:\n",
    "                dataset_leak_copy['annotations'][j]['image_id'] = img_id\n",
    "                dataset_leak_copy['annotations'][j]['id'] = anno_id\n",
    "                new_annotations_all.append(dataset_leak_copy['annotations'][j])\n",
    "                anno_id += 1\n",
    "        dataset_leak_copy['images'][i]['id'] = img_id\n",
    "        new_images_all.append(dataset_leak_copy['images'][i])  \n",
    "        img_id += 1\n",
    "del new_annotations_all[267] # segment labeling 안되어있는것 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "id": "a84c0b26-b3dc-48a4-841f-fee40c729316",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2617 195 2812\n",
      "2812\n",
      "20988 691 21679\n",
      "21679\n",
      "3272 195 3467\n",
      "3467\n",
      "26240 691 26931\n",
      "26931\n"
     ]
    }
   ],
   "source": [
    "# train\n",
    "dataset_train_new_images = dataset_train['images'] + new_images\n",
    "# print(len(dataset_train['images']),len(new_images),len(dataset_train['images'])+len(new_images))\n",
    "# print(len(dataset_train_new_images))\n",
    "\n",
    "dataset_train_new_annotations = dataset_train['annotations'] + new_annotations\n",
    "# print(len(dataset_train['annotations']),len(new_annotations),len(dataset_train['annotations'])+len(new_annotations))\n",
    "# print(len(dataset_train_new_annotations))\n",
    "\n",
    "# train_all\n",
    "dataset_train_all_new_images = dataset_train_all['images'] + new_images_all\n",
    "# print(len(dataset_train_all['images']),len(new_images_all),len(dataset_train_all['images'])+len(new_images_all))\n",
    "# print(len(dataset_train_all_new_images))\n",
    "\n",
    "dataset_train_all_new_annotations = dataset_train_all['annotations'] + new_annotations_all\n",
    "# print(len(dataset_train_all['annotations']),len(new_annotations_all),len(dataset_train_all['annotations'])+len(new_annotations_all))\n",
    "# print(len(dataset_train_all_new_annotations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "id": "f258372d-48a4-42aa-bb5a-288e74052d89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2812\n",
      "21679\n",
      "3467\n",
      "26931\n"
     ]
    }
   ],
   "source": [
    "dataset_train['images'] = dataset_train_new_images\n",
    "dataset_train['annotations'] = dataset_train_new_annotations\n",
    "# print(len(dataset_train['images']))\n",
    "# print(len(dataset_train['annotations']))\n",
    "\n",
    "dataset_train_all['images'] = dataset_train_all_new_images\n",
    "dataset_train_all['annotations'] = dataset_train_all_new_annotations\n",
    "# print(len(dataset_train_all['images']))\n",
    "# print(len(dataset_train_all['annotations']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "id": "b7934ce6-6c5e-406e-8f12-568fd41a866a",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path1 = '../data/train_new.json'\n",
    "with open(save_path1, 'w', encoding='utf-8') as f:\n",
    "    json.dump(dataset_train, f)\n",
    "\n",
    "save_path2 = '../data/train_all_new.json'\n",
    "with open(save_path2, 'w', encoding='utf-8') as f:\n",
    "    json.dump(dataset_train_all, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

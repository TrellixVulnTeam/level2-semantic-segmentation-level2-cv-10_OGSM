{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "300637fe-da37-45e4-8965-5b1b38c9c25f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from pycocotools.coco import COCO\n",
    "import os\n",
    "import os.path as osp\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "3f532f61-f7e9-4319-97c1-bd5c6e1b6e41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.01s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.51s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "data_dir = '../data'\n",
    "train_path = osp.join(data_dir, 'train.json')\n",
    "train_all_path = osp.join(data_dir, 'train_all.json')\n",
    "org_test_path = osp.join(data_dir, 'test_original.json')\n",
    "new_test_path = osp.join(data_dir, 'test.json')\n",
    "leak_path = osp.join(data_dir, 'batch_03', 'data.json')\n",
    "\n",
    "##### train.json #####\n",
    "with open(train_path, 'r') as f:\n",
    "    train_json = json.load(f)\n",
    "\n",
    "##### train_all.json #####\n",
    "with open(train_all_path, 'r') as f:\n",
    "    train_all_json = json.load(f)\n",
    "    \n",
    "##### test_original.json (original) #####\n",
    "with open(org_test_path, 'r') as f:\n",
    "    org_test_json = json.load(f)\n",
    "    \n",
    "##### test.json (new) #####\n",
    "with open(new_test_path, 'r') as f:\n",
    "    new_test_json = json.load(f)\n",
    "\n",
    "##### data.json (leakage) #####\n",
    "with open(leak_path, 'r') as f:\n",
    "    data_json = json.load(f)\n",
    "    \n",
    "coco_org = COCO(org_test_path) # test_original.json\n",
    "coco_new = COCO(new_test_path) # test.json\n",
    "coco_leak = COCO(leak_path) # data.json\n",
    "\n",
    "new_train_json = copy.deepcopy(train_json)\n",
    "new_train_all_json = copy.deepcopy(train_all_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "595122e7-cb1d-47a3-b86e-ece7a3c8ac87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Original number of images]: 819\n",
      "[New number of images]: 624\n",
      "[Number of leakages]: 195\n"
     ]
    }
   ],
   "source": [
    "org_img_fpaths = [img_info['file_name'] for img_info in org_test_json['images']]\n",
    "new_img_fpaths = [img_info['file_name'] for img_info in new_test_json['images']]\n",
    "leak_img_fpaths = sorted(list(set(org_img_fpaths) - set(new_img_fpaths)))\n",
    "\n",
    "print(\"[Original number of images]: {}\".format(len(org_img_fpaths)))\n",
    "print(\"[New number of images]: {}\".format(len(new_img_fpaths)))\n",
    "print(\"[Number of leakages]: {}\".format(len(leak_img_fpaths)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "392e824a-1876-413a-a4e8-28d5c78105ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [1] 유출된 image가 segmentation이 있는지 확인\n",
    "    # [1.1] data.json (leak_json)에서 leak_img_fpaths에 있는 file_name에 해당하는 img_id 가져오기 [0,10,334,...]\n",
    "    # [1.2] 각 img_id에 해당하는 annotation의 length > 0 이면 data.json 에서 'images'하고 'annotations' 가져와서 합칠 data structure 만들기\n",
    "# [2] train.json과 train_all.json에 합치기\n",
    "\n",
    "leak_img_infos = [img_info for img_info in data_json['images'] if img_info['file_name'] in leak_img_fpaths]\n",
    "leak_img_ids = [img_info['id'] for img_info in leak_img_infos]\n",
    "\n",
    "# DS for Extra data\n",
    "new_train_img_infos = []\n",
    "new_train_ann_infos = []\n",
    "new_train_all_img_infos = []\n",
    "new_train_all_ann_infos = []\n",
    "\n",
    "# Last index for image_id and ann_id of original train.json / train_all.json\n",
    "train_json_img_next_id = train_json['images'][-1]['id'] + 1\n",
    "train_json_ann_next_id = train_json['annotations'][-1]['id'] + 1\n",
    "train_all_json_img_next_id = train_all_json['images'][-1]['id'] + 1\n",
    "train_all_json_ann_next_id = train_all_json['annotations'][-1]['id'] + 1\n",
    "\n",
    "for img_id, img_info in zip(leak_img_ids, leak_img_infos):\n",
    "    ann_ids = coco_leak.getAnnIds(img_id)\n",
    "    ann_infos = coco_leak.loadAnns(ann_ids)\n",
    "    \n",
    "    for ann_info in ann_infos:\n",
    "        if ann_info['category_id'] == 0 or len(ann_info['segmentation']) == 0:\n",
    "            continue\n",
    "        # Annotations\n",
    "        copy1 = copy.deepcopy(ann_info)\n",
    "        copy2 = copy.deepcopy(ann_info)\n",
    "        \n",
    "        copy1.update({\n",
    "            'id': train_json_ann_next_id,\n",
    "            'image_id': train_json_img_next_id\n",
    "        })\n",
    "        \n",
    "        copy2.update({\n",
    "            'id': train_all_json_ann_next_id,\n",
    "            'image_id': train_all_json_img_next_id\n",
    "        })\n",
    "        \n",
    "        new_train_ann_infos.append(copy1)\n",
    "        new_train_all_ann_infos.append(copy2)\n",
    "        \n",
    "        train_json_ann_next_id += 1\n",
    "        train_all_json_ann_next_id += 1\n",
    "    \n",
    "    # Images\n",
    "    img_copy1 = copy.deepcopy(img_info)\n",
    "    img_copy2 = copy.deepcopy(img_info)\n",
    "    \n",
    "    img_copy1.update({'id':train_json_img_next_id})\n",
    "    img_copy2.update({'id':train_all_json_img_next_id})\n",
    "    \n",
    "    new_train_img_infos.append(img_copy1)\n",
    "    new_train_all_img_infos.append(img_copy2)\n",
    "    \n",
    "    train_json_img_next_id += 1\n",
    "    train_all_json_img_next_id += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "87aaa7bc-59ec-4c96-b02e-0b008b9cfcc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Number of new images for train.json]: 195\n",
      "[Number of new annotations for train.json]: 691\n",
      "[Number of new images for train_all.json]: 195\n",
      "[Number of new annotations for train_all.json]: 691\n"
     ]
    }
   ],
   "source": [
    "print(\"[Number of new images for train.json]:\", len(new_train_img_infos))\n",
    "print(\"[Number of new annotations for train.json]:\", len(new_train_ann_infos))\n",
    "print(\"[Number of new images for train_all.json]:\", len(new_train_all_img_infos))\n",
    "print(\"[Number of new annotations for train_all.json]:\", len(new_train_all_ann_infos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "4f054ebc-3106-45af-ac28-9158731e3bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_train_json['images'].extend(new_train_img_infos)\n",
    "new_train_json['annotations'].extend(new_train_ann_infos)\n",
    "\n",
    "new_train_all_json['images'].extend(new_train_all_img_infos)\n",
    "new_train_all_json['annotations'].extend(new_train_all_ann_infos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "b4d665ad-7fbc-4001-a30c-45821b07f31e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------\n",
      "[Number of bonus images for train.json]: 195\n",
      "[Number of bonus annotations for train.json]: 691\n",
      "[Number of bonus images for train_all.json]: 195\n",
      "[Number of bonus annotations for train_all.json]: 691\n",
      "------------------------------------------------------------\n",
      "[Number of original images for train.json]: 2617\n",
      "[Number of original annotations for train.json]: 20988\n",
      "[Number of original images for train_all.json]: 3272\n",
      "[Number of original annotations for train_all.json]: 26240\n",
      "------------------------------------------------------------\n",
      "[Number of combined images for train.json]: 2812\n",
      "[Number of combined annotations for train.json]: 21679\n",
      "[Number of combined images for train_all.json]: 3467\n",
      "[Number of combined annotations for train_all.json]: 26931\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"-\"*60)\n",
    "print(\"[Number of bonus images for train.json]:\", len(new_train_img_infos))\n",
    "print(\"[Number of bonus annotations for train.json]:\", len(new_train_ann_infos))\n",
    "print(\"[Number of bonus images for train_all.json]:\", len(new_train_all_img_infos))\n",
    "print(\"[Number of bonus annotations for train_all.json]:\", len(new_train_all_ann_infos))\n",
    "print(\"-\"*60)\n",
    "print(\"[Number of original images for train.json]:\", len(train_json['images']))\n",
    "print(\"[Number of original annotations for train.json]:\", len(train_json['annotations']))\n",
    "print(\"[Number of original images for train_all.json]:\", len(train_all_json['images']))\n",
    "print(\"[Number of original annotations for train_all.json]:\", len(train_all_json['annotations']))\n",
    "print(\"-\"*60)\n",
    "print(\"[Number of combined images for train.json]:\", len(new_train_json['images']))\n",
    "print(\"[Number of combined annotations for train.json]:\", len(new_train_json['annotations']))\n",
    "print(\"[Number of combined images for train_all.json]:\", len(new_train_all_json['images']))\n",
    "print(\"[Number of combined annotations for train_all.json]:\", len(new_train_all_json['annotations']))\n",
    "print(\"-\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3d379c9-8ec3-4f84-b54a-e7d0320628ae",
   "metadata": {},
   "source": [
    "> # Save new json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "63153bb1-71cf-44fe-89fc-c2bcee776211",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_json_save_path = '../data/train_new.json'\n",
    "train_all_json_save_path = '../data/train_all_new.json'\n",
    "\n",
    "with open(train_json_save_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(new_train_json, f)\n",
    "    \n",
    "with open(train_all_json_save_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(new_train_all_json, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32602c08-7ca3-400c-a164-4d75e3b092cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85090dff-d0f0-4cea-bf73-0405ea174ced",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
